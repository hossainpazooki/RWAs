# Regulatory Knowledge Engineering Workbench

**A Computational Law Platform for MiCA, RWA Tokenization, and Stablecoin Frameworks**

Transforms regulatory documents into executable knowledge through ontology extraction, declarative rules, and traceable decision logic.

---

## Why This Exists

Financial regulation is complex, multi-jurisdictional, and constantly evolving. Traditional compliance relies on legal memos and manual interpretationâ€”approaches that don't scale and can't be audited systematically.

This system takes a different approach: encode regulations as *executable rules* with full traceability back to source legal text. Each decision produces a machine-readable trace showing exactly which provisions applied and why. This enables:

- **Automated compliance checking** against real regulatory frameworks (MiCA, DLT Pilot, GENIUS Act)
- **Knowledge engineering workflows** for legal teams to model, verify, and maintain rules
- **Semantic consistency verification** ensuring rules faithfully represent source provisions
- **Gap analysis** identifying legal provisions without corresponding rule coverage

The system currently models the EU's Markets in Crypto-Assets Regulation (MiCA), the DLT Pilot Regime, an illustrative RWA tokenization framework, and the proposed US GENIUS Act for stablecoin oversight.

---

## Architecture

```mermaid
flowchart TB
    subgraph Corpus["Legal Corpus"]
        LC1[MiCA 2023]
        LC2[DLT Pilot 2022]
        LC3[GENIUS Act 2025]
        LC4[RWA Framework]
    end

    subgraph Core["Core Engine"]
        ONT[Ontology Layer]
        DSL[Rule DSL]
        DE[Decision Engine]
        CE[Consistency Engine]
    end

    subgraph Production["Production Layer"]
        DB[(Database)]
        COMP[Compiler]
        IDX2[Premise Index]
        RT[Runtime]
        CACHE[IR Cache]
    end

    subgraph RAG["Internal RAG"]
        IDX[Document Index]
        CTX[Context Retrieval]
    end

    subgraph UI["Interfaces"]
        API[FastAPI]
        ST[Streamlit Workbench]
        CH[Charts]
    end

    Corpus --> IDX
    IDX --> CTX
    CTX --> CE
    ONT --> DSL
    DSL --> DE
    DSL --> COMP
    COMP --> DB
    COMP --> IDX2
    DB --> RT
    IDX2 --> RT
    CACHE --> RT
    DE --> API
    RT --> API
    CE --> API
    API --> ST
    ST --> CH
```

### Component Summary

| Component | Purpose | Key Modules |
|-----------|---------|-------------|
| **Legal Corpus** | Normalized excerpts of source regulations | `data/legal/mica_2023/`, `genius_act_2025/`, etc. |
| **Ontology** | Typed domain model (Actor, Instrument, Activity, Provision) | `backend/ontology/`, `ocaml/core/ontology.ml` |
| **Rule DSL** | YAML-based declarative rules with decision trees | `backend/rules/`, `ocaml/core/rule_dsl.ml` |
| **Decision Engine** | Evaluates scenarios, produces traces and obligations | `backend/rules/engine.py` |
| **Consistency Engine** | Tier 0-4 verification of rules against source text | `backend/verify/consistency_engine.py` |
| **Persistence** | SQLite database for rules, verifications, reviews | `backend/persistence/` |
| **Compiler** | Compiles rules to IR for efficient execution | `backend/compiler/` |
| **Runtime** | Linear IR evaluation with O(1) rule lookup | `backend/runtime/` |
| **Internal RAG** | Context retrieval for KE workflows (not public Q&A) | `backend/rag/rule_context.py` |
| **KE Workbench** | Streamlit UI for rule inspection and review | `frontend/ke_dashboard.py` |
| **Charts** | Interactive tree visualizations | `backend/visualization/`, `frontend/pages/charts.py` |

---

## Key Features

- **Multi-rulebook support** â€” MiCA (EU crypto-assets), RWA tokenization, DLT Pilot Regime, GENIUS Act (US stablecoins)
- **Executable rules with decision traces** â€” Every evaluation produces a step-by-step trace linking back to source provisions
- **Tiered semantic consistency checks** â€” Tier 0 (schema), Tier 1 (lexical), Tier 2-4 (semantic/NLI, stub)
- **Production-grade architecture** â€” Compiled IR, O(1) rule lookup via premise index, linear evaluation
- **Database persistence** â€” SQLite (PostgreSQL-compatible) for rules, verification results, and human reviews
- **Internal RAG for legal context** â€” Source text retrieval, related provisions, coverage gap detection
- **KE workbench** â€” Decision tree viewer, evidence panel, review queue, analytics dashboard
- **Interactive charts** â€” Rulebook outline, ontology browser, corpus-rule links, legal corpus coverage
- **Pure Python deployment** â€” Runs on Streamlit Cloud without OCaml compilation

---

## KE Workbench User Guide

The Knowledge Engineering (KE) Workbench is a Streamlit application for inspecting, verifying, and reviewing regulatory rules.

### Workbench Layout

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  âš–ï¸ KE Workbench                           [ğŸ” Verify All] [â†º Reset]        â”‚
â”‚  Workflow: Queue â†’ Review â†’ Test â†’ Submit                                   â”‚
â”‚  ğŸ“Š 7 rules | âœ“ 0 verified | âš  7 needs review | âœ— 0 inconsistent           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              â”‚                                    â”‚                         â”‚
â”‚  ğŸ“‹ RULES    â”‚     RULE DETAILS                   â”‚  ğŸ“– CONTEXT             â”‚
â”‚              â”‚                                    â”‚                         â”‚
â”‚  â—‹ Queue     â”‚  mica_art36_public_offer_auth      â”‚  mica_2023 Art. 36      â”‚
â”‚  â— Navigator â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                         â”‚
â”‚              â”‚  â”‚Decision â”‚Trace/   â”‚Analytics â”‚  â”‚  â–¼ Primary text         â”‚
â”‚  â–¼ Mica 2023 â”‚  â”‚Tree     â”‚Test     â”‚          â”‚  â”‚  "Any person intending  â”‚
â”‚    ? rule_1  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   to offer crypto-      â”‚
â”‚    ? rule_2  â”‚                                    â”‚   assets..."            â”‚
â”‚  â–¼ Rwa 2025  â”‚  [Tree visualization or           â”‚                         â”‚
â”‚    ? rule_3  â”‚   test interface or               â”‚  â–¼ Related provisions   â”‚
â”‚              â”‚   analytics charts]               â”‚  â€¢ Art. 37 (0.82)       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚                                    â”‚  â€¢ Art. 48 (0.71)       â”‚
â”‚  ğŸ“Š Insights â”‚                                    â”‚                         â”‚
â”‚  Total: 7    â”‚                                    â”‚                         â”‚
â”‚  Verified: 0 â”‚                                    â”‚                         â”‚
â”‚              â”‚                                    â”‚                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Workflow Steps

**Step 1: Select a Rule**
- Use **Queue** view to see rules prioritized by verification status
- Use **Navigator** view to browse by document hierarchy
- Status indicators: `?` needs review, `âœ“` verified, `âœ—` inconsistent

**Step 2: Review Decision Tree**
- Visual tree shows rule's decision logic
- Toggle **Overlay** to see consistency status on each node
- Green = pass, Yellow = warning, Red = fail

**Step 3: Run Trace Test**
- Enter test scenario values in the **Trace/Test** tab
- Click **Run Trace** to execute rule against scenario
- View step-by-step evaluation path
- Check if decision matches expected behavior

**Step 4: Review Analytics**
- **Analytics** tab shows verification evidence
- Pie chart: Pass/Warn/Fail distribution
- Evidence table: Tier, Category, Label, Score, Details
- Confidence score: Weighted average of all checks

**Step 5: Submit Review**
- If rule behaves correctly, mark as verified
- If issues found, mark as inconsistent with notes

### Left Panel: Rules Selection

| Mode | Description | Use When |
|------|-------------|----------|
| **Queue** | Rules sorted by urgency | Processing review backlog |
| **Navigator** | Hierarchical by document | Exploring specific regulation |

**Queue Filters:**
- Status: `needs_review`, `unverified`, `inconsistent`, `all`
- Document: Filter by source document (MiCA, RWA, etc.)

**Navigator Structure:**
```
â–¼ Mica 2023 (4)
  ? mica_art36_public_offer_authorization (Art.36(1))
  ? mica_art38_reserve_assets (Art.38)
  ? mica_art48_emt_authorization (Art.48)
  âœ“ mica_art45_significant_art (Art.45)
â–¼ Rwa Eu 2025 (3)
  ...
```

### Center Panel: Rule Details

#### Decision Tree Tab

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   check_exemption   â”‚
                    â”‚ is_credit_inst==T   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
                      TRUE         FALSE
                         â”‚           â”‚
                    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
                    â”‚ exempt  â”‚ â”‚  check  â”‚
                    â”‚         â”‚ â”‚  auth   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                                â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
                             TRUE      FALSE
                                â”‚         â”‚
                           â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â” â”Œâ”€â”€â”´â”€â”€â”
                           â”‚permittedâ”‚ â”‚deny â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜
```

- **[Verify]** - Run consistency checks on this rule
- **[Overlay â˜‘]** - Color nodes by consistency status

#### Trace/Test Tab

| Field | Input | Description |
|-------|-------|-------------|
| `instrument_type` | Dropdown | `art`, `emt`, `stablecoin`, etc. |
| `activity` | Dropdown | `public_offer`, `admission_trading`, etc. |
| `jurisdiction` | Text | `EU`, `US`, etc. |
| `is_credit_institution` | Checkbox | Whether actor is licensed bank |
| `authorized` | Checkbox | Whether already authorized |

**Expected Outputs:**
- **Decision**: `authorized`, `not_authorized`, `exempt`, etc.
- **Trace Table**: Node â†’ Condition â†’ Result (âœ“/âœ—) â†’ Value

#### Analytics Tab

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pass: 6    Fail: 0    Warn: 6    Conf: 78% â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚         â”‚   50%    â”‚ 50%   â–  Pass           â”‚
â”‚         â”‚  Green   â”‚Yellow â”‚ â–  Warning      â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚ â–  Fail         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Evidence Details                           â”‚
â”‚  Tier â”‚ Category         â”‚ Label â”‚ Score   â”‚
â”‚  â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚  0    â”‚ schema_valid     â”‚ pass  â”‚ 100%    â”‚
â”‚  0    â”‚ required_fields  â”‚ pass  â”‚ 100%    â”‚
â”‚  1    â”‚ deontic_align    â”‚ warn  â”‚ 60%     â”‚
â”‚  1    â”‚ keyword_overlap  â”‚ pass  â”‚ 85%     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Right Panel: Context

- **Primary text**: Source legal provision text
- **Document metadata**: Citation, jurisdiction, article reference
- **Related provisions**: Similar rules ranked by similarity score

### Header Actions

| Button | Action | Effect |
|--------|--------|--------|
| **Verify All** | Run Tier 0-1 checks on all rules | Updates all verification statuses |
| **Reset** | Clear all session state | Returns to initial view |

### Quick Stats

The **Insights** section shows:
- **Total Rules**: Count of all loaded rules
- **Needs Review**: Rules with warnings or unverified
- **Verified**: Rules marked as consistent
- **Inconsistent**: Rules with failed checks
- **Progress bar**: Percentage verified

### Charts Page

Access via sidebar navigation to see:
- **Rulebook Outline**: Hierarchical view of legal corpus with coverage
- **Ontology Browser**: Actor/Instrument/Activity type hierarchy
- **Corpus Links**: Document â†’ Article â†’ Rule traceability

### Production Architecture Demo

Access via sidebar navigation (`Production Demo`) to explore:

| Section | What It Shows |
|---------|---------------|
| **Architecture Overview** | Side-by-side comparison of traditional O(n) vs production O(1) lookup |
| **Compile Rules** | Transform YAML rules to compiled IR with premise keys |
| **Premise Index** | Inverted index statistics and contents (field:value â†’ rule_ids) |
| **IR Cache** | Cache hit/miss rates and memory statistics |
| **Performance Comparison** | Real-time benchmarking of single scenario evaluation |
| **Batch Evaluation** | Compare throughput for 10-100 scenarios |

**Demo Workflow:**
1. Click **Compile All Rules** to build IR and premise index
2. Configure a test scenario (instrument type, jurisdiction, activity)
3. Click **Run Performance Comparison** to see speedup metrics
4. Try **Batch Evaluation** to see throughput at scale

---

## Getting Started

### Prerequisites

- Python 3.11+
- Git

### Installation

```bash
# Clone the repository
git clone https://github.com/hossainpazooki/RWAs.git
cd RWAs

# Create virtual environment
python -m venv .venv

# Activate (Windows PowerShell)
.\.venv\Scripts\Activate

# Activate (macOS/Linux)
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### Run Tests

```bash
pytest tests/ -v
```

### Launch KE Workbench

```bash
streamlit run frontend/ke_dashboard.py
```

The workbench opens at `http://localhost:8501` with:
- Rule selection and decision tree visualization
- Consistency evidence panel
- Source context and related provisions
- Charts page for rulebook/coverage analysis

### Run API Server (Optional)

```bash
uvicorn backend.main:app --reload
```

API available at `http://localhost:8000` with endpoints:
- `POST /decide` â€” Evaluate scenario against rules
- `GET /rules` â€” List loaded rules
- `GET /ke/*` â€” Internal KE endpoints
- `POST /v2/migrate` â€” Migrate YAML rules to database
- `POST /v2/rules/compile` â€” Compile all rules to IR
- `POST /v2/rules/{id}/evaluate` â€” Evaluate with compiled IR
- `POST /v2/evaluate` â€” Batch evaluation with O(1) lookup

### Optional Dependencies

```bash
# ML features (vector search, semantic similarity)
pip install -r requirements-ml.txt

# Visualization enhancements
pip install -r requirements-visualization.txt
```

---

## Repository Structure

```
RWAs/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ ontology/          # Domain types (Actor, Instrument, Provision, etc.)
â”‚   â”œâ”€â”€ rules/             # YAML rule files + decision engine
â”‚   â”‚   â”œâ”€â”€ mica_authorization.yaml   # MiCA public offer rules (3 rules)
â”‚   â”‚   â”œâ”€â”€ mica_stablecoin.yaml      # MiCA stablecoin/ART/EMT (6 rules)
â”‚   â”‚   â”œâ”€â”€ genius_stablecoin.yaml    # GENIUS Act US stablecoins (6 rules)
â”‚   â”‚   â””â”€â”€ rwa_authorization.yaml    # RWA tokenization (2 rules)
â”‚   â”œâ”€â”€ verify/            # Semantic consistency engine
â”‚   â”œâ”€â”€ analytics/         # Error patterns, drift detection
â”‚   â”œâ”€â”€ rag/               # Internal retrieval (BM25, context)
â”‚   â”œâ”€â”€ visualization/     # Tree adapters, chart rendering
â”‚   â””â”€â”€ api/               # FastAPI routes
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ ke_dashboard.py    # Main Streamlit app
â”‚   â””â”€â”€ pages/             # Charts, Review Queue
â”œâ”€â”€ ocaml/
â”‚   â””â”€â”€ core/              # OCaml ontology + rule DSL (source of truth)
â”œâ”€â”€ data/
â”‚   â””â”€â”€ legal/             # Legal corpus (MiCA, DLT Pilot, GENIUS)
â”œâ”€â”€ docs/                  # Design documentation
â”œâ”€â”€ tests/                 # Test suite (375+ tests)
â””â”€â”€ requirements.txt
```

---

## Conceptual Layers

### Layer 1-2: Ontology & Rule DSL (OCaml + Python)

The formal type system for regulatory knowledge:

- **Ontology types**: `Actor`, `Instrument`, `Activity`, `Provision`, `Obligation`
- **Relation types**: `IMPOSES_OBLIGATION_ON`, `PERMITS`, `PROHIBITS`, `EXEMPTS`
- **Rule DSL**: YAML schema with `applies_if` conditions and `decision_tree` logic
- OCaml source in `ocaml/core/`, Python mirrors in `backend/ontology/`

### Layer 3A: Decision Engine

Deterministic rule evaluation with full traceability:

- **RuleLoader**: Parses YAML rules into executable structures
- **DecisionEngine**: Evaluates scenarios against applicable rules
- **TraceStep**: Records each condition evaluation for explainability
- See `backend/rules/engine.py`

### Layer 3B: Semantic Consistency Engine

Automated verification of rules against source legal text:

| Tier | Status | Description |
|------|--------|-------------|
| 0 | Implemented | Schema validation, required fields, date consistency |
| 1 | Implemented | Deontic alignment, keyword overlap, negation checks |
| 2 | Stub | Semantic similarity (requires sentence-transformers) |
| 3 | Stub | NLI entailment checking |
| 4 | Stub | Cross-rule consistency |

See `backend/verify/consistency_engine.py` and [Semantic Consistency Spec](docs/semantic_consistency_regulatory_kg.md).

### Layer 4: Internal RAG

Context retrieval for KE workflows (not public-facing):

- **Document indexing**: BM25 with optional vector embeddings
- **Source retrieval**: Get legal text backing a rule
- **Related provisions**: Find similar rules with structural filtering
- **Coverage gaps**: Identify legal text without mapped rules
- See `backend/rag/rule_context.py`

### Layer 5: KE Interfaces

Tools for knowledge engineers:

- **Streamlit workbench**: Decision tree viewer, evidence panel, review queue
- **Charts**: Rulebook outline, ontology browser, corpus coverage
- **FastAPI /ke endpoints**: Programmatic access to verification and analytics
- See `frontend/ke_dashboard.py`, `backend/api/routes_ke.py`

---

## Rulebooks Modeled

| Document ID | Framework | Jurisdiction | Status | Example Rules |
|-------------|-----------|--------------|--------|---------------|
| `mica_2023` | Markets in Crypto-Assets (MiCA) | EU | Modeled | `mica_art36_public_offer_authorization`, `mica_art38_reserve_assets` |
| `rwa_eu_2025` | RWA Tokenization | EU | Illustrative | `rwa_tokenization_authorization`, `rwa_custody_requirements` |
| `dlt_pilot_2022` | DLT Pilot Regime | EU | Corpus only | Future rule modeling planned |
| `genius_act_2025` | GENIUS Act (Stablecoins) | US | Illustrative | Based on proposed bill; some provisions fictionalized |

**Note**: MiCA rules are based on the published regulation. RWA and GENIUS rules are illustrative models for demonstration purposes.

---

## How to Extend

### Add a New Rulebook

1. Create legal corpus entry in `data/legal/{document_id}/`:
   - `meta.yaml` with document metadata
   - `text_normalized.txt` with normalized excerpts

2. Create rule file in `backend/rules/{document_id}.yaml`

3. Map rules to corpus via `source.document_id`

### Add Rules to Existing Rulebook

1. Edit the appropriate YAML file in `backend/rules/`
2. Follow the [Rule DSL specification](docs/rule_dsl.md)
3. Run `pytest tests/test_rules.py -v` to validate

### Modify Semantic Checks

1. Edit `backend/verify/consistency_engine.py`
2. Add new check methods following existing patterns
3. Update tests in `tests/test_consistency_engine.py`

### Documentation

- Update `docs/*.md` when changing ontology, DSL, or engine behavior
- Keep `CLAUDE.md` current for AI assistant context

---

## Status & Disclaimers

**This is a research/demo project, not legal advice.**

- Rules are interpretive models of regulatory text, not authoritative legal guidance
- The GENIUS Act rulebook is based on a proposed bill and includes fictionalized provisions
- Coverage is illustrativeâ€”not all provisions from source documents are modeled
- Always consult qualified legal counsel for compliance decisions

---

## Documentation

- [Knowledge Model](docs/knowledge_model.md) â€” Ontology design, type definitions, worked examples
- [Rule DSL](docs/rule_dsl.md) â€” YAML rule specification, operators, decision trees
- [Engine Design](docs/engine_design.md) â€” KE workbench architecture, layer descriptions
- [Semantic Consistency](docs/semantic_consistency_regulatory_kg.md) â€” Verification tiers, evidence structures

---

## License

MIT License. See [LICENSE](LICENSE) for details.

---

## Credits

Built with assistance from [Claude Code](https://claude.ai/code) (Anthropic).

Regulatory frameworks referenced:
- [MiCA - Regulation (EU) 2023/1114](https://eur-lex.europa.eu/eli/reg/2023/1114/oj)
- [DLT Pilot - Regulation (EU) 2022/858](https://eur-lex.europa.eu/eli/reg/2022/858/oj)
- [GENIUS Act - S.394 (118th Congress)](https://www.congress.gov/bill/118th-congress/senate-bill/394)
